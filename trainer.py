"""è®­ç»ƒå™¨æ¨¡å—"""

import math
from typing import Optional, List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler
from tqdm import tqdm

from model import set_frozen_batchnorm_eval


# =============================================================================
# ã€è¯Šæ–­å·¥å…·å‡½æ•°ã€‘ç”¨äºæ£€æµ‹æ•°æ®/ç‰¹å¾åç¼©é—®é¢˜
# =============================================================================

def check_batch_diversity(
    data: torch.Tensor,
    sample_ids: List[str],
    batch_idx: int,
    max_print: int = 16,
) -> dict:
    """
    ã€Batch æ•°æ®å¤šæ ·æ€§æ£€æŸ¥ã€‘
    
    æ£€æŸ¥ batch å†…æ ·æœ¬æ˜¯å¦çœŸæ­£ä¸åŒï¼ˆæ’é™¤ Dataset/DataLoader é‡å¤æ ·æœ¬ bugï¼‰
    
    Args:
        data: è¾“å…¥æ•°æ® [B, C, D, H, W]
        sample_ids: æ ·æœ¬ ID åˆ—è¡¨
        batch_idx: å½“å‰ batch ç´¢å¼•
        max_print: æœ€å¤šæ‰“å°çš„æ ·æœ¬æ•°
    
    Returns:
        dict: è¯Šæ–­ç»“æœ
    """
    B = data.shape[0]
    device = data.device
    
    result = {
        "batch_size": B,
        "unique_ids": len(set(sample_ids)),
        "is_duplicate_suspected": False,
        "data_stats": {},
    }
    
    # ä»…åœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°è¯¦ç»†ä¿¡æ¯
    if batch_idx == 0:
        print("\n" + "=" * 70)
        print("[DIAG] Batch æ•°æ®å¤šæ ·æ€§æ£€æŸ¥ (batch_idx=0)")
        print("=" * 70)
        
        # 1. æ‰“å°æ ·æœ¬ ID
        print(f"\n[1] æ ·æœ¬ ID (å‰ {min(max_print, B)} ä¸ª):")
        for i, sid in enumerate(sample_ids[:max_print]):
            print(f"  [{i:2d}] {sid}")
        
        unique_count = len(set(sample_ids))
        print(f"\n  Unique IDs: {unique_count}/{B}")
        if unique_count < B:
            print(f"  âš ï¸ è­¦å‘Š: å­˜åœ¨é‡å¤æ ·æœ¬ IDï¼")
            result["is_duplicate_suspected"] = True
        
        # 2. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç»Ÿè®¡é‡
        print(f"\n[2] æ¯ä¸ªæ ·æœ¬çš„è¾“å…¥ç»Ÿè®¡ (shape={data.shape}):")
        
        # Per-sample mean/std [B]
        data_flat = data.view(B, -1)  # [B, C*D*H*W]
        per_sample_mean = data_flat.mean(dim=1)  # [B]
        per_sample_std = data_flat.std(dim=1)    # [B]
        
        print(f"  Per-sample mean: min={per_sample_mean.min().item():.6f}, "
              f"max={per_sample_mean.max().item():.6f}, "
              f"std={per_sample_mean.std().item():.6f}")
        print(f"  Per-sample std:  min={per_sample_std.min().item():.6f}, "
              f"max={per_sample_std.max().item():.6f}, "
              f"std={per_sample_std.std().item():.6f}")
        
        # 3. è®¡ç®—ä¸æ ·æœ¬ 0 çš„ L2 è·ç¦»
        print(f"\n[3] ä¸æ ·æœ¬ 0 çš„ L2 è·ç¦»:")
        sample0 = data_flat[0:1]  # [1, C*D*H*W]
        l2_to_0 = torch.norm(data_flat - sample0, dim=1)  # [B]
        
        print(f"  L2 distances to sample[0]: {l2_to_0[:min(10, B)].tolist()}")
        print(f"  L2 min (excl. self): {l2_to_0[1:].min().item():.6f}" if B > 1 else "  N/A")
        print(f"  L2 max: {l2_to_0.max().item():.6f}")
        print(f"  L2 mean (excl. self): {l2_to_0[1:].mean().item():.6f}" if B > 1 else "  N/A")
        
        # 4. åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤æ•°æ®
        if B > 1 and l2_to_0[1:].max().item() < 1e-5:
            print(f"\n  ğŸš¨ ä¸¥é‡è­¦å‘Š: æ‰€æœ‰æ ·æœ¬ä¸æ ·æœ¬ 0 å‡ ä¹ç›¸åŒ (L2 < 1e-5)!")
            print(f"     è¿™è¡¨æ˜ Dataset/DataLoader è¿”å›äº†é‡å¤æ ·æœ¬ï¼")
            result["is_duplicate_suspected"] = True
        
        result["data_stats"] = {
            "per_sample_mean_std": per_sample_mean.std().item(),
            "l2_to_0_min": l2_to_0[1:].min().item() if B > 1 else 0,
            "l2_to_0_max": l2_to_0.max().item(),
        }
        
        print("=" * 70 + "\n")
    
    return result


def check_feature_diversity(
    features: torch.Tensor,
    batch_idx: int,
    source: str = "backbone",
    warn_threshold: float = 0.95,
) -> dict:
    """
    ã€ç‰¹å¾å¤šæ ·æ€§æ£€æŸ¥ã€‘
    
    æ£€æŸ¥ backbone è¾“å‡ºç‰¹å¾æ˜¯å¦å‘ç”Ÿåç¼©ï¼ˆæ‰€æœ‰æ ·æœ¬ç‰¹å¾å‡ ä¹ç›¸åŒï¼‰
    
    Args:
        features: ç‰¹å¾å¼ é‡ [B, C]
        batch_idx: å½“å‰ batch ç´¢å¼•
        source: ç‰¹å¾æ¥æºåç§°
        warn_threshold: ä½™å¼¦ç›¸ä¼¼åº¦è­¦å‘Šé˜ˆå€¼
    
    Returns:
        dict: è¯Šæ–­ç»“æœ
    """
    B, C = features.shape
    device = features.device
    
    result = {
        "shape": (B, C),
        "is_collapsed": False,
        "cosine_offdiag_mean": None,
        "feat_var_across_batch": None,
    }
    
    if B < 2:
        return result
    
    # 1. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
    feat_norm = F.normalize(features, p=2, dim=1)  # [B, C]
    cos_sim = torch.mm(feat_norm, feat_norm.t())   # [B, B]
    
    # æ’é™¤å¯¹è§’çº¿
    mask = ~torch.eye(B, dtype=torch.bool, device=device)
    off_diag = cos_sim[mask]
    
    cos_mean = off_diag.mean().item()
    cos_min = off_diag.min().item()
    cos_max = off_diag.max().item()
    
    result["cosine_offdiag_mean"] = cos_mean
    result["cosine_offdiag_min"] = cos_min
    result["cosine_offdiag_max"] = cos_max
    
    # 2. è®¡ç®—ç‰¹å¾æ–¹å·®
    feat_var = features.var(dim=0).mean().item()  # è·¨ batch çš„æ–¹å·®
    result["feat_var_across_batch"] = feat_var
    
    # 3. ä¸æ ·æœ¬ 0 çš„ L2 è·ç¦»
    l2_to_0 = torch.norm(features - features[0:1], dim=1)  # [B]
    result["l2_to_0"] = l2_to_0.tolist()
    
    # 4. åˆ¤æ–­æ˜¯å¦åç¼©
    if cos_mean > warn_threshold and feat_var < 0.01:
        result["is_collapsed"] = True
    
    # ä»…åœ¨ç¬¬ä¸€ä¸ª batch æ‰“å°è¯¦ç»†ä¿¡æ¯
    if batch_idx == 0:
        print(f"\n" + "-" * 60)
        print(f"[DIAG] ç‰¹å¾å¤šæ ·æ€§æ£€æŸ¥ ({source})")
        print(f"-" * 60)
        print(f"  Shape: {features.shape}")
        print(f"  Cosine similarity (off-diag):")
        print(f"    mean={cos_mean:.6f}, min={cos_min:.6f}, max={cos_max:.6f}")
        print(f"  Feature variance across batch: {feat_var:.6f}")
        print(f"  L2 to sample[0] (first 5): {l2_to_0[:5].tolist()}")
        
        if result["is_collapsed"]:
            print(f"\n  ğŸš¨ ä¸¥é‡è­¦å‘Š: ç‰¹å¾å‘ç”Ÿåç¼©ï¼")
            print(f"     cos_mean={cos_mean:.4f} > {warn_threshold}, feat_var={feat_var:.6f} < 0.01")
            print(f"     å¯èƒ½åŸå› :")
            print(f"       1. Batch ç»´åº¦è¢«é”™è¯¯èšåˆï¼ˆå¦‚ mean(dim=0) è€Œé mean(dim=(2,3,4))")
            print(f"       2. CLS token å–é”™ï¼ˆå¦‚ x[0] è€Œé x[:, 0])")
            print(f"       3. ç‰¹å¾è¢« expand/repeat å¤åˆ¶")
            print(f"       4. Dataset è¿”å›é‡å¤æ ·æœ¬")
        
        print(f"-" * 60 + "\n")
    
    return result


def diagnose_first_batch(
    model: nn.Module,
    data: torch.Tensor,
    target: torch.Tensor,
    sample_ids: List[str],
    device: torch.device,
    batch_idx: int = 0,
) -> dict:
    """
    ã€å®Œæ•´çš„ç¬¬ä¸€ä¸ª batch è¯Šæ–­ã€‘
    
    åœ¨è®­ç»ƒå¼€å§‹æ—¶å¯¹ç¬¬ä¸€ä¸ª batch è¿›è¡Œå…¨é¢è¯Šæ–­
    """
    if batch_idx != 0:
        return {}
    
    print("\n" + "=" * 70)
    print("[DIAG] è®­ç»ƒç¬¬ä¸€ä¸ª Batch å®Œæ•´è¯Šæ–­")
    print("=" * 70)
    
    result = {}
    
    # 1. æ£€æŸ¥è¾“å…¥æ•°æ®å¤šæ ·æ€§
    result["batch_check"] = check_batch_diversity(data, sample_ids, batch_idx)
    
    # 2. æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print(f"\n[DIAG] æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥:")
    print(f"  Labels dtype: {target.dtype}")
    print(f"  Labels unique values: {target.unique().tolist()}")
    print(f"  Labels distribution: {torch.bincount(target, minlength=4).tolist()}")
    
    # æ–­è¨€æ ‡ç­¾æ­£ç¡®æ€§
    assert target.dtype == torch.long, f"Labels dtype åº”ä¸º torch.long, å®é™…ä¸º {target.dtype}"
    assert target.min() >= 0 and target.max() <= 3, f"Labels åº”åœ¨ [0,3], å®é™…èŒƒå›´ [{target.min()}, {target.max()}]"
    
    # 3. æ£€æŸ¥ backbone ç‰¹å¾
    with torch.no_grad():
        backbone_features = model.backbone(data)
    
    result["backbone_check"] = check_feature_diversity(
        backbone_features, batch_idx, source="backbone", warn_threshold=0.95
    )
    
    # 4. æ£€æŸ¥ logits
    with torch.no_grad():
        logits = model(data, return_features=False)
    
    print(f"\n[DIAG] Logits æ£€æŸ¥:")
    print(f"  Shape: {logits.shape}")
    print(f"  Per-sample std (mean): {logits.std(dim=1).mean().item():.6f}")
    print(f"  Across-sample std: {logits.std(dim=0).tolist()}")
    
    # æ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„ logits
    print(f"\n  å‰ 5 ä¸ªæ ·æœ¬çš„ logits:")
    for i in range(min(5, logits.shape[0])):
        logit_str = ", ".join([f"{v:.4f}" for v in logits[i].tolist()])
        pred = logits[i].argmax().item()
        true = target[i].item()
        print(f"    [{i}] [{logit_str}] -> pred={pred}, true={true}")
    
    # 5. æ±‡æ€»è¯Šæ–­
    print("\n" + "=" * 70)
    print("[DIAG] è¯Šæ–­æ±‡æ€»")
    print("=" * 70)
    
    issues = []
    
    if result["batch_check"].get("is_duplicate_suspected"):
        issues.append("Dataset/DataLoader å¯èƒ½è¿”å›é‡å¤æ ·æœ¬")
    
    if result["backbone_check"].get("is_collapsed"):
        issues.append("Backbone ç‰¹å¾å‘ç”Ÿåç¼©")
    
    if logits.std(dim=0).mean().item() < 0.05:
        issues.append("Logits è·¨æ ·æœ¬æ–¹å·®æå°ï¼Œæ¨¡å‹å¯èƒ½åªè¾“å‡ºå›ºå®šå€¼")
    
    if issues:
        print("\nğŸš¨ æ£€æµ‹åˆ°ä»¥ä¸‹é—®é¢˜:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
    else:
        print("\nâœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„æ•°æ®/ç‰¹å¾åç¼©é—®é¢˜")
    
    print("=" * 70 + "\n")
    
    return result


def check_optimizer_config(
    optimizer: torch.optim.Optimizer,
    model: nn.Module,
) -> None:
    """
    ã€ä¼˜åŒ–å™¨é…ç½®æ£€æŸ¥ã€‘
    
    ç¡®ä¿ optimizer åŒ…å«æ­£ç¡®çš„å‚æ•°ç»„
    """
    print("\n" + "-" * 60)
    print("[DIAG] ä¼˜åŒ–å™¨é…ç½®æ£€æŸ¥")
    print("-" * 60)
    
    total_params = 0
    trainable_params = 0
    
    for name, param in model.named_parameters():
        total_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    
    print(f"  æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {100.0 * trainable_params / total_params:.2f}%")
    
    print(f"\n  ä¼˜åŒ–å™¨å‚æ•°ç»„:")
    opt_total = 0
    for i, group in enumerate(optimizer.param_groups):
        group_params = sum(p.numel() for p in group['params'] if p.requires_grad)
        opt_total += group_params
        name = group.get('name', f'group_{i}')
        lr = group.get('lr', 'N/A')
        print(f"    [{i}] {name}: {group_params:,} params, lr={lr}")
    
    # æ–­è¨€æ£€æŸ¥
    if opt_total == 0:
        raise RuntimeError(
            "ğŸš¨ ä¼˜åŒ–å™¨å‚æ•°ç»„ä¸ºç©ºï¼æ²¡æœ‰å¯è®­ç»ƒå‚æ•°è¢«æ·»åŠ åˆ°ä¼˜åŒ–å™¨ä¸­ã€‚\n"
            "è¯·æ£€æŸ¥ setup_parameter_freezing å’Œ create_optimizer å‡½æ•°ã€‚"
        )
    
    if opt_total != trainable_params:
        print(f"\n  âš ï¸ è­¦å‘Š: ä¼˜åŒ–å™¨å‚æ•° ({opt_total:,}) != å¯è®­ç»ƒå‚æ•° ({trainable_params:,})")
    
    # æ£€æŸ¥ head æ˜¯å¦åœ¨ä¼˜åŒ–å™¨ä¸­
    head_in_opt = False
    for group in optimizer.param_groups:
        if group.get('name') == 'head':
            head_in_opt = True
            break
    
    if not head_in_opt:
        print(f"\n  âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°åä¸º 'head' çš„å‚æ•°ç»„")
    
    print("-" * 60 + "\n")


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(
        self, patience: int = 10, save_path: str = "model.pth", is_main: bool = True
    ):
        self.patience = patience
        self.counter = 0
        self.best_score: Optional[float] = None
        self.early_stop = False
        self.save_path = save_path
        self.is_main = is_main

    def __call__(self, val_loss: float, model: nn.Module):
        if self.best_score is None:
            self.best_score = val_loss
            self.save_checkpoint(model)
        elif val_loss >= self.best_score:
            self.counter += 1
            if self.is_main:
                print(f"æ—©åœè®¡æ•°å™¨: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = val_loss
            self.save_checkpoint(model)
            self.counter = 0

    def save_checkpoint(self, model: nn.Module):
        if self.is_main:
            torch.save(model.state_dict(), self.save_path)
            print(f"æœ€ä½³æ¨¡å‹ä¿å­˜è‡³ {self.save_path}")


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    scaler: Optional[GradScaler],
    max_grad_norm: float = 1.0,
    global_loss_fn: Optional[nn.Module] = None,
    lambda_global: float = 1.0,
    gradient_accumulation_steps: int = 1,
    enable_memory_efficient: bool = False,
) -> Tuple[float, float]:
    """å•ä¸ª epoch çš„è®­ç»ƒ"""
    model.train()
    set_frozen_batchnorm_eval(model)

    running_loss = 0.0
    running_ce_loss = 0.0
    running_global_loss = 0.0
    running_corrects = 0
    total = 0
    valid_used = 0
    valid_total = 0
    any_global_loss_computed = False

    pbar = tqdm(train_loader, desc="è®­ç»ƒ")
    amp_enabled = (scaler is not None) and (device.type == "cuda")
    autocast_device_type = "cuda" if device.type == "cuda" else "cpu"

    first_batch_diagnosed = False
    
    for batch_idx, batch_data in enumerate(pbar):
        # Unpack data
        data, target, sample_ids, age, measures = batch_data

        # Handle two-view batch logic
        two_view_batch = data.ndim == 6 and data.size(1) == 2
        
        # ã€è¯Šæ–­ã€‘ç¬¬ä¸€ä¸ª batch è¿›è¡Œå®Œæ•´è¯Šæ–­ï¼ˆåœ¨ two_view å±•å¼€ä¹‹å‰ï¼‰
        if batch_idx == 0 and not first_batch_diagnosed:
            first_batch_diagnosed = True
            # å¦‚æœæ˜¯ two_viewï¼Œå…ˆå–ç¬¬ä¸€ä¸ªè§†å›¾è¿›è¡Œè¯Šæ–­
            diag_data = data[:, 0] if two_view_batch else data
            diag_data = diag_data.to(device, non_blocking=True)
            diag_target = target.to(device, non_blocking=True)
            
            diagnose_first_batch(
                model=model,
                data=diag_data,
                target=diag_target,
                sample_ids=list(sample_ids) if not isinstance(sample_ids, list) else sample_ids,
                device=device,
                batch_idx=0,
            )
        if two_view_batch:
            B0 = data.size(0)
            target_base = target
            sample_ids_base = sample_ids

            # Flatten views: [B,2,C,D,H,W] -> [2B,C,D,H,W]
            data = data.view(B0 * 2, *data.shape[2:])
            target = target.repeat_interleave(2)
            sample_ids = [sid for sid in sample_ids for _ in range(2)]

            if global_loss_fn is not None:
                age = age.repeat_interleave(2)
                # Correctly handle measures dimensions:
                # If measures is [B, 2, K, 3], flatten to [2B, K, 3]
                if measures.ndim == 4 and measures.shape[1] == 2:
                    measures = measures.view(B0 * 2, *measures.shape[2:])
                else:
                    measures = measures.repeat_interleave(2, dim=0)
        else:
            target_base = None

        data = data.to(device, non_blocking=True)
        target = target.to(device, non_blocking=True)

        use_global = global_loss_fn is not None
        if use_global:
            age = age.to(device, non_blocking=True)
            measures = measures.to(device, non_blocking=True)

        # Gradient accumulation reset
        if batch_idx % gradient_accumulation_steps == 0:
            optimizer.zero_grad(set_to_none=True)

        is_accum_end = ((batch_idx + 1) % gradient_accumulation_steps == 0) or (
            (batch_idx + 1) == len(train_loader)
        )

        features = None
        with torch.autocast(device_type=autocast_device_type, enabled=amp_enabled):
            if use_global:
                logits, features = model(data, return_features=True)
            else:
                logits = model(data, return_features=False)

            ce_loss = criterion(logits, target)
        
        # ã€è¿è¡Œæ—¶æ£€æŸ¥ã€‘å‘¨æœŸæ€§æ£€æµ‹ç‰¹å¾åç¼©ï¼ˆæ¯ 100 ä¸ª batch æ£€æŸ¥ä¸€æ¬¡ï¼‰
        if batch_idx > 0 and batch_idx % 100 == 0:
            with torch.no_grad():
                backbone_feat = model.backbone(data)
                if backbone_feat.shape[0] > 1:
                    feat_norm = F.normalize(backbone_feat, p=2, dim=1)
                    cos_sim = torch.mm(feat_norm, feat_norm.t())
                    mask = ~torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)
                    cos_mean = cos_sim[mask].mean().item()
                    
                    if cos_mean > 0.98:
                        print(f"\nâš ï¸ [batch={batch_idx}] ç‰¹å¾åç¼©è­¦å‘Š: cos_sim_offdiag_mean={cos_mean:.4f} > 0.98")

        # Compute Global Loss if enabled
        if use_global:
            assert features is not None
            global_loss, was_computed = _compute_global_loss(
                global_loss_fn,
                features.float(),
                age.float(),
                measures.float(),
                sample_ids,
                batch_idx,
                device,
                autocast_device_type=autocast_device_type,
                amp_enabled=False,  # Force FP32 for stability
            )
            any_global_loss_computed = any_global_loss_computed or was_computed
            loss = ce_loss + lambda_global * global_loss
        else:
            global_loss = torch.tensor(0.0, device=device, dtype=torch.float32)
            loss = ce_loss

        loss = loss / gradient_accumulation_steps

        # Backward pass
        if scaler is not None:
            scaler.scale(loss).backward()
            if is_accum_end:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                scaler.step(optimizer)
                scaler.update()
        else:
            loss.backward()
            if is_accum_end:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()

        # Memory cleanup
        if enable_memory_efficient:
            del data
            if features is not None:
                del features
            if device.type == "cuda" and (batch_idx % 50 == 0):
                torch.cuda.empty_cache()
        
        # Calculate metrics
        if two_view_batch:
            # Average logits across views for accuracy
            logits_acc = logits.view(B0, 2, -1).mean(dim=1)  # [B0, num_classes]
            preds = logits_acc.argmax(dim=1)
            batch_size = target_base.size(0)
            correct = preds.eq(target_base.to(preds.device)).sum().item()
        else:
            preds = logits.argmax(dim=1)
            batch_size = target.size(0)
            correct = preds.eq(target).sum().item()

        total += batch_size
        running_loss += loss.item() * batch_size * gradient_accumulation_steps
        running_ce_loss += ce_loss.item() * batch_size
        running_global_loss += global_loss.item() * batch_size
        running_corrects += correct

        if len(optimizer.param_groups) > 1:
            postfix = {
                "lr_head": f"{optimizer.param_groups[0]['lr']:.2e}",
                "lr_back": f"{optimizer.param_groups[1]['lr']:.2e}",
                "loss": f"{running_loss / total:.4f}",
                "ce": f"{running_ce_loss / total:.4f}",
                "global": f"{running_global_loss / total:.4f}",
                "acc": f"{100.0 * running_corrects / total:.2f}%",
            }
        else:
            postfix = {
                "lr": f"{optimizer.param_groups[0]['lr']:.2e}",
                "loss": f"{running_loss / total:.4f}",
                "ce": f"{running_ce_loss / total:.4f}",
                "global": f"{running_global_loss / total:.4f}",
                "acc": f"{100.0 * running_corrects / total:.2f}%",
            }
        if global_loss_fn is not None and valid_total > 0:
            postfix["valid"] = f"{100.0 * valid_used / valid_total:.1f}%"
        pbar.set_postfix(postfix)

    if global_loss_fn is not None and not any_global_loss_computed:
        raise RuntimeError(
            "Global Loss å·²å¯ç”¨ï¼Œä½†æ•´ä¸ª epoch å†…ä»æœªçœŸæ­£è®¡ç®—ã€‚"
            "è¯·æ£€æŸ¥ batch_size æ˜¯å¦ >=2ï¼Œæ•°æ®é›†æ˜¯å¦å·²è¿‡æ»¤ç¼ºå¤± age/measuresã€‚"
        )

    return running_loss / total, running_corrects / total * 100.0


def _compute_global_loss(
    global_loss_fn: nn.Module,
    features: torch.Tensor,
    age: torch.Tensor,
    measures: torch.Tensor,
    sample_ids: List[str],
    batch_idx: int,
    device: torch.device,
    autocast_device_type: str,
    amp_enabled: bool,
) -> Tuple[torch.Tensor, bool]:
    """è®¡ç®— Global Loss

    Returns:
        Tuple[torch.Tensor, bool]: (loss, was_computed)
            - loss: Global Loss å€¼
            - was_computed: æ˜¯å¦çœŸæ­£è®¡ç®—äº† Global Loss
    """
    measures = torch.nan_to_num(measures, nan=0.0, posinf=0.0, neginf=0.0)

    # Handle measures dimensions
    # Single view: [B, K, 3] -> [B, K, 3]
    # Two view (flattened): [2B, K, 3] -> [2B, K, 3]
    # Incorrectly flattened: [B, 2, K, 3] -> flatten to [2B, K, 3]
    
    if measures.ndim == 4 and measures.shape[1] == 2:
        measures = measures.view(-1, *measures.shape[2:])
    
    if measures.ndim != 3:
         raise RuntimeError(f"Global Loss enabled but measures dim invalid (expected 3, got {measures.ndim}).")
    
    if measures.shape[1] <= 0:
        raise RuntimeError(
            f"Global Loss enabled but ROI count is 0 (shape={measures.shape}).\n"
            "Check config.measure_root and config.region_order_json."
        )

    if torch.isnan(age).any():
        bad_ids = [
            sid for sid, ok in zip(sample_ids, (~torch.isnan(age)).tolist()) if not ok
        ]
        raise RuntimeError(f"Global Loss enabled but NaN age found: {bad_ids[:10]}")

    zero_mask = measures.abs().sum(dim=(1, 2)) == 0
    if bool(zero_mask.any()):
        bad_ids = [sid for sid, z in zip(sample_ids, zero_mask.tolist()) if z]
        raise RuntimeError(f"Global Loss enabled but all-zero measures found: {bad_ids[:10]}")

    batch_size = age.size(0)
    if batch_idx == 0 and batch_size < 2:
        raise RuntimeError("Global Loss enabled but first batch_size < 2.")

    valid_mask = ~torch.isnan(age)
    cur_valid = int(valid_mask.sum().item())

    if cur_valid > 1:
        # Fix mismatch between flattened measures and non-flattened age in validation
        # If validation measures are [2B, ...] (due to dataset logic) but age is [B],
        # we only take the first view to align with age.
        if measures.shape[0] == age.shape[0] * 2:
             measures = measures.view(age.shape[0], 2, *measures.shape[1:])[:, 0, ...]
        
        valid_features = features[valid_mask].float()
        valid_ages = age[valid_mask].float()
        valid_measures = measures[valid_mask].float()
        with torch.autocast(device_type=autocast_device_type, enabled=amp_enabled):
            loss = global_loss_fn(valid_features, valid_ages, valid_measures)
            return loss, True  # æˆåŠŸè®¡ç®—

    return torch.tensor(0.0, device=device, dtype=torch.float32), False  # æœªè®¡ç®—


@torch.no_grad()
def validate_epoch(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    device: torch.device,
    is_main: bool,
    enable_memory_efficient: bool = False,
    global_loss_fn: Optional[nn.Module] = None,
    lambda_global: float = 1.0,
    debug_first_batch: bool = False,
) -> Tuple[float, float, float, List[int], List[int], List[str], List[List[float]]]:
    """éªŒè¯/æµ‹è¯•é˜¶æ®µ"""
    model.eval()
    running_loss = 0.0
    running_ce_loss = 0.0
    running_global_loss = 0.0
    running_corrects = 0
    total = 0

    all_preds = []
    all_targets = []
    all_logits = []
    all_sample_ids = []

    for batch_idx, batch_data in enumerate(
        tqdm(loader, desc="éªŒè¯" if is_main else "val", leave=False)
    ):
        data, target, sample_id, age, measures = batch_data
        data = data.to(device, non_blocking=True)

        if batch_idx == 0:
            print("data.shape =", data.shape, "dtype =", data.dtype)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®åˆ†å¸ƒ
            if debug_first_batch and is_main:
                print("\n" + "="*60)
                print("[DEBUG] ç¬¬ä¸€ä¸ª batch è¯Šæ–­ä¿¡æ¯")
                print("="*60)
                print(f"è¾“å…¥æ•°æ®ç»Ÿè®¡:")
                print(f"  - min: {data.min().item():.6f}")
                print(f"  - max: {data.max().item():.6f}")
                print(f"  - mean: {data.mean().item():.6f}")
                print(f"  - std: {data.std().item():.6f}")

        target = target.to(device, non_blocking=True)

        use_global = global_loss_fn is not None
        if use_global:
            age = age.to(device, non_blocking=True)
            measures = measures.to(device, non_blocking=True)

        # æ ¹æ®æ˜¯å¦éœ€è¦ global loss å†³å®šæ˜¯å¦è¿”å› features
        features = None
        if use_global:
            logits, features = model(data, return_features=True)
        else:
            logits = model(data, return_features=False)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ç‰¹å¾å’Œ logits åˆ†å¸ƒ
        if batch_idx == 0 and debug_first_batch and is_main:
            # è·å– backbone ç‰¹å¾
            with torch.no_grad():
                backbone_features = model.backbone(data)
                
            print(f"\nBackbone ç‰¹å¾ç»Ÿè®¡:")
            print(f"  - shape: {backbone_features.shape}")
            print(f"  - min: {backbone_features.min().item():.6f}")
            print(f"  - max: {backbone_features.max().item():.6f}")
            print(f"  - mean: {backbone_features.mean().item():.6f}")
            print(f"  - std: {backbone_features.std().item():.6f}")
            
            # æ£€æŸ¥ç‰¹å¾æ˜¯å¦åç¼©ï¼ˆæ‰€æœ‰æ ·æœ¬ç‰¹å¾å‡ ä¹ç›¸åŒï¼‰
            if backbone_features.shape[0] > 1:
                # è®¡ç®—æ ·æœ¬é—´ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦
                feat_norm = backbone_features / (backbone_features.norm(dim=1, keepdim=True) + 1e-8)
                cos_sim = torch.mm(feat_norm, feat_norm.t())
                # æ’é™¤å¯¹è§’çº¿
                mask = ~torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)
                off_diag_sim = cos_sim[mask]
                print(f"  - æ ·æœ¬é—´ä½™å¼¦ç›¸ä¼¼åº¦ (mean): {off_diag_sim.mean().item():.6f}")
                print(f"  - æ ·æœ¬é—´ä½™å¼¦ç›¸ä¼¼åº¦ (min): {off_diag_sim.min().item():.6f}")
                print(f"  - æ ·æœ¬é—´ä½™å¼¦ç›¸ä¼¼åº¦ (max): {off_diag_sim.max().item():.6f}")
                
                if off_diag_sim.mean().item() > 0.99:
                    print(f"  \u26a0\ufe0f è­¦å‘Š: ç‰¹å¾é«˜åº¦ç›¸ä¼¼ï¼Œå¯èƒ½å‘ç”Ÿç‰¹å¾åå¡ï¼")
            
            print(f"\nLogits ç»Ÿè®¡:")
            print(f"  - shape: {logits.shape}")
            print(f"  - min: {logits.min().item():.6f}")
            print(f"  - max: {logits.max().item():.6f}")
            print(f"  - mean: {logits.mean().item():.6f}")
            print(f"  - std: {logits.std().item():.6f}")
            
            # æ‰“å°æ¯ä¸ªæ ·æœ¬çš„ logits
            print(f"\nå‰ 5 ä¸ªæ ·æœ¬çš„ logits:")
            for i in range(min(10, logits.shape[0])):
                logit_str = ", ".join([f"{v:.4f}" for v in logits[i].tolist()])
                pred = logits[i].argmax().item()
                true = target[i].item()
                print(f"  æ ·æœ¬ {i}: [{logit_str}] -> pred={pred}, true={true}")
            
            # æ£€æŸ¥ logits æ˜¯å¦å‡ ä¹ç›¸åŒ
            if logits.shape[0] > 1:
                logits_std_per_sample = logits.std(dim=1)  # æ¯ä¸ªæ ·æœ¬å†…éƒ¨ logits çš„æ ‡å‡†å·®
                logits_std_across_samples = logits.std(dim=0)  # è·¨æ ·æœ¬çš„æ ‡å‡†å·®
                print(f"\nLogits æ–¹å·®åˆ†æ:")
                print(f"  - æ¯ä¸ªæ ·æœ¬å†…éƒ¨ logits æ ‡å‡†å·® (mean): {logits_std_per_sample.mean().item():.6f}")
                print(f"  - è·¨æ ·æœ¬çš„ logits æ ‡å‡†å·®: {logits_std_across_samples.tolist()}")
                
                if logits_std_per_sample.mean().item() < 0.01:
                    print(f"  \u26a0\ufe0f è­¦å‘Š: æ¯ä¸ªæ ·æœ¬çš„ logits æ–¹å·®æå°ï¼Œæ¨¡å‹è¾“å‡ºå‡ ä¹å‡åŒ€åˆ†å¸ƒï¼")
            
            print("="*60 + "\n")

        ce_loss = criterion(logits, target)

        # è®¡ç®— global lossï¼ˆä¸è®­ç»ƒæ—¶å¯¹é½ï¼‰
        if use_global:
            assert features is not None  # ç”±äº use_global=True æ—¶å¿…æœ‰ features
            global_loss, _ = _compute_global_loss(
                global_loss_fn,
                features.float(),
                age.float(),
                measures.float(),
                sample_id,
                batch_idx,
                device,
                autocast_device_type="cuda" if device.type == "cuda" else "cpu",
                amp_enabled=False,
            )
            loss = ce_loss + lambda_global * global_loss
        else:
            global_loss = torch.tensor(0.0, device=device, dtype=torch.float32)
            loss = ce_loss

        preds = logits.argmax(dim=1)
        bs = data.size(0)
        total += bs
        running_loss += loss.item() * bs
        running_ce_loss += ce_loss.item() * bs
        running_global_loss += global_loss.item() * bs
        running_corrects += preds.eq(target).sum().item()

        # ç«‹å³ç§»åˆ°CPUä»¥é‡Šæ”¾GPUå†…å­˜
        all_preds.extend(preds.detach().cpu().numpy().tolist())
        all_targets.extend(target.detach().cpu().numpy().tolist())
        all_logits.extend(logits.detach().cpu().numpy().tolist())
        all_sample_ids.extend(sample_id)

        # å†…å­˜ä¼˜åŒ–ï¼šåŠæ—¶é‡Šæ”¾GPUå†…å­˜
        if enable_memory_efficient:
            del data, logits, preds, target
            if features is not None:
                del features
            if batch_idx % 10 == 0 and device.type == "cuda":
                torch.cuda.empty_cache()

    return (
        running_loss / total,
        running_ce_loss / total,
        running_corrects / total * 100.0,
        all_preds,
        all_targets,
        all_sample_ids,
        all_logits,
    )


def create_lr_scheduler(
    optimizer: torch.optim.Optimizer,
    num_epochs: int,
    warmup_epochs: int,
) -> torch.optim.lr_scheduler.LambdaLR:
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆwarmup + ä½™å¼¦ï¼‰"""

    def lr_lambda(epoch: int) -> float:
        if epoch < warmup_epochs:
            return float(epoch + 1) / float(max(1, warmup_epochs))

        progress = float(epoch - warmup_epochs) / float(
            max(1, num_epochs - warmup_epochs)
        )
        cosine = 0.5 * (1.0 + math.cos(math.pi * progress))
        min_factor = 1e-2  # é™ä½æœ€å°å­¦ä¹ ç‡åˆ° 1%
        return min_factor + (1.0 - min_factor) * cosine

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)
